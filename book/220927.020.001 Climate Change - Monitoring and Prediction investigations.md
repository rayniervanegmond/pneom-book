# Climate Change Monitoring Data Gathering

This project concerns the illustration of how we can use Python programming to gather information that can be used to monitor climate change through data gathering and processing.

The source for the publication's outline is one of the ARSET courses that are provided by ARSET. The course is ["course name"]() and presentation and video material can be fond at ["course website"](). 

The reason for creating the this course material is that while the information provided in the ARSET courses is of extremely high value and quality they often linger on the theoretical level. It is for interested parties that want to start in this climate change modeling and management domain hard to get started with work in this field.

To actually gather and process such remote sensing and model data there are a number of moving parts that need to be combined in just the right manner to do any work. This requires a level of technical expertise that these domain experts (often) lack. 

The idea with this particular publication is that by providing the domain experts or interested parties with a step-by-step guideline for setting up a technical infrastructure that will support such data gathering and analysis will lower the bar to start using this great resource of training and modeling data.

> Note that this publication is part of a series of publications that aim to implemnet running code for  many of the ARSET and EUMETSAT and other agencies' training materials.

## Outline Of The Publication
The 

## Overview of Climate Change
The 

## Earth Observations in Climate Change Assessment
As I mentioned in the outline discussion the use of earth observation is part of a pipeline that aims to fulfil a purpose. No research is done (or should be done) in a vacuum. Especially in the climate change context there is the objective of "preventing or reducing the negative impact of environmental/meteorological changes on society."

In this context we can understand that doing a backwards requirements analysis:
* we need to understand potential impacts of climate change on society.
* we need to understand the overall systems and what influences the different aspects of society.
* we need to understand the short-term (sub-seasonal) impacts on daily life by environmental factors.
* we need to understand what larger processes drive these short-term impacts.
* we need to get data to hypothesise what these processes might be.


### The NASA Remote Sensing Satellite System

As was explained above there are a number of domains that the satellite remote sensing and climate modelling communities focus on. These areas are followed by many

#### About missions, spacecrafts and instruments
For newcommers to the field of satellite based remote sensing there is a lot of new terminology to acquire. While we learn of new launches of satellites through the news and are told that "...the new LANDSAT satellite will be measuring ..." there are a few things ommitted in that one sentence. Perhaps not of too great impartance for the general publica but if one is interested in working with data gatherd by satellite-based remote sensing systems we need to use these terms with a little more care.

The various agencies, like: NASA, EUMETSAT, and others, have defined missions that aims to support a specific data gathering need as part of a larger societal objective. Like we addressed earlier in this publication, information is gathered to provide researchers and decision makers with data to set policies.

For instance if society is concerned about the deterioration of global and regional air quality it might want information to see how air quality is distributed across the globe and how it develops over time. The researchers who understand the chemics of air quality might state thhat they need to have data on different aspects of the air like: chemical molecules, particulate matter concentrations at any given place and time in the atmosphere. This is then encapsulated in a requirements statement for a mission that will develop one or more satellites with the right instruments to measure these atmospheric components.

A similar description could be given for the policy makers might have the question "How will sea-levels rise over the next hundred years?" Their objective is to understand risks and posisble ways to mitigate the risk and prepare/addapt to such sea-level changes. To answer this question researchers will pose their hypothesis of such possible changes based on physics (e.g. physical properties of saline waters), climate parameters such as surface temperatures, sources of water, hydrological cycle and meteorology and past and current sealevels. Again to collect the data to feed into the models a remote sensing mission is defined that aims to provide data to asnwer these questions. This will drive the decision about what instruments to develop and how these instruments will be placed in orbit by allocating them on one or more satellites.

There is another source of information available to policy makers. If we think back to our discussion of information gathering for  policy decision making we remember that we went from source observations, hypothesising models and calculating model results, decision process input to policy setting. Each of these steps provides additional information to the decision making process. The production of models and creating model result datasets is another very important source of information. Clearly --because the models are part of a decision making pipeline-- they are often geared to support answering specific questions. So while we might make observations of physical characteristics of the Earth (which is **data**) it is the modeling that converts data into **information** that is used by policy makers. 

Depending on the type of information this process might consult the different data sources that were generated by their instruments. So to make policy around global and regional sealevel rise the information is generated from climate models, hydrological models, geophysical models etc. These models consume sea, land and atmospheric temperature measurements, water level altimetry measurements, surface gravitational measurements, digital elevation measurements etc. Each of these measurements is provided by one or more instruments flown on one or more satellites that are managed by one or more agency missions.

From these short descriptions it is clear that if a researcher wants data he/she needs to know what information is needed, what measurements support producing the information:

1. where **information** do I need to answer a policy question?
1. which **model-results** are available that provide the information I need"
1. what **physical measurement types** can support creating the model results?
1. which **instruments produce data** for these types of physical measurements?
1. what are the **past and current missions** that operate these instruments?
1. which **satellites** flown by these missions?
1. where are the **data-sets collected** by these satellites?
1. how do I **access and extract data points** of interest from the data-sets?

Now if we look at this from an IT perspective --the purpose of this publication is to enable domain experts/IT personnel-- to create data pipelines to serve this information we move from the opposite direction. Typically each mission will disseminate the collected data sets in their own way. Unfortunately there is no common platform or API that covers all collected observational data and modeling results.

The data access solution developer is given a set of sources for model results and measurements (steps 2 and 8) and creates the software to access the information. Eventhough these data distribution sites might have many different model results and data series, the access process to them is often the same. In case of the NASA and EUMETSAT agencies they even support single-sign-on procedures where access to the different sources is authorized by a single account. This greatly simplifies getting access to the required data.

In later sections of this document I will go into the details of how to get access to the agencies data distribution channels. (see chapter ...) 

#### The Domain

As should be clear from the discussion in the previous chapter it is the "mission" that provides the information for a particular policy area. The entrypoint into find the information is the domain question: water resource management, fire risk management, sealevel rise impact mitigation and adaptation. The policy drives the information need, the information need drives the models, the models drive the mission, the mission drives the instruments, the instruments drive the satellites, the satellites drive the data series.

The ARSET training that this publication is based off is called "**ARSET - Introduction to NASA Resources for Climate Change Applications**"and the part for which it provides a software tutorial is called  "**Climate Change Monitoring & Impacts Using Remote Sensing and Modeled Data**".

We see that this is taking an "inside-out approach" in that it looks at the data and informations source available and discusses how these sources can support answering questions in the various domains. The domain addrfess in particular are:

* The impact of greenhouse gasses on global warming 
* The impact of global warming on global and regional sealevel rise
* The impact of [ocean warming, shrinking ice sheets, glacial retreat, decreased snow cover] on regional sealevel rise
* The impact of global warming on local climate trends
* The impact of global warming on extreme weather events

#### The NASA remote sensing missions
These different topics of interest are driving the definition of the agencency missions. With respect to NASA we have the following missions that aim to support the decision processes of policy makers.

1. Greenhouse Gasses and Atmospheric data:
   * **AURA mission [Earthdata](https://disc.gsfc.nasa.gov/datasets?page=1&project=Aura)**: provides data on ozone, aerosols, and key gases to gain insights into the chemistry of our atmosphere
   * **OCO-2 mission [Earthdata](https://disc.gsfc.nasa.gov/datasets?page=1&project=OCO-2)**: (Orbiting Carbon Observatory-2) provides data on global levels of atmospheric CO2 with the precision, resolution, and coverage needed to characterize sources and sinks on regional scales.
   * **CALIPSO mission [ASDC](https://asdc.larc.nasa.gov/data/CALIPSO/)**: provides data from spaceborne lidar, combined with passive imagery, will lead to insights in the role aerosols and clouds play in regulating the Earth’s climate.
   * **CLOUDSAT mission [EarthData](https://disc.gsfc.nasa.gov/datasets?page=1&source=CloudSat%20CloudSat-CPR)**: provides data global survey of the vertical structure and overlap of cloud systems and their liquid- and ice-water contents.
   * **AQUA mission [AQUA](https://disc.gsfc.nasa.gov/datasets?page=1&project=Aqua)**: provides multispectral imagery and data on the amount of water vapor in a column of the atmosphere and the vertical distribution of temperature, water vapor and aerosols wrt Earth’s energy budget. [go to access tutorial]( #access-aqua-and-terra-missions-datasets)

1. Water Resources Monitoring Data:
   * **GPM-mission [EarthData](https://disc.gsfc.nasa.gov/datasets?page=1&project=GPM)**: (Global Precipitation Measurement Mission) provides data with a constellation of additional satellites that together provide next-generation global observations of precipitation from space.
   * **GRACE and GRACE-FO mission: [NASA JPL](https://disc.gsfc.nasa.gov/datasets?page=1&project=GRACE-DA-DM)**:(Gravity Recovery and Climate Experiment and follow-on) provide data on  water movement to monitor changes in underground water storage, the amount of water in large lakes and rivers, soil moisture, ice sheets and glaciers, and sea level.
   * **ICESat-2 misssion [NSIDC](https://nsidc.org/data/icesat-2/data-sets)**: (Ice, Cloud, and land Elevation Satellite-2) provides data on the elevation of ice sheets, glaciers, and sea ice.
   * **Jason-3 mission [NOAA NCEI](https://www.ncei.noaa.gov/products/jason-satellite-products)**: provides data on the height of the ocean’s surface using radar altimetry going back to 1992.
   * **Sentinel-6 Michael Freilich mission [NASA JPL](https://podaac.jpl.nasa.gov/Sentinel-6?tab=datasets)**: A joint NAS/ESA effort that provides the most accurate data yet on sea level and how it changes over time.
   * **SMAP mission [NSIDC](https://nsidc.org/data/smap/data)**: (Soil Moisture Active Passive) mission provides data on the amount of water in the surface soil everywhere on Earth.
2. Earth Surface and Combined Realm Monitoring Data:
   * **Terra mission**: provides global data on the state of the atmosphere, land, and oceans, and their interactions with solar radiation and with one another.  
   * **Suomi-NPP mission [EarthData](https://disc.gsfc.nasa.gov/datasets?page=1&project=SUOMI-NPP)**: (Suomi National Polar-orbiting Partnership) provides data on climate and operational weather and atmospheric, land surface, and sea surface temperatures critical to understanding the longterm dynamics of climate change.
   * **Landsat 7, 8, and 9 missions**: provide long-term information on the coastal regions, polar ice, islands, and continental areas characterize and monitor landcover use and change over time. The main purpose of the LANDSAT missions is to provide generic multi-spectral imagery of the earth. <br/>
   [go to access tutorial](#access-landsat-missions-datasets).

#### The Global Climate Models

As we mentioned earlier the information needs by policy makers are first and formost met by the results of model analysis based on the observational data collected in the missions described above. Models typically have a specific focus and with climate change models these typically are: Global Climate Change and Atmosphere, Global Climate Change and Sealevel, Global Climate Change and Temperature. 

Often these large global scale climate change models are fed by the results from more specific domain related models (which might ingest the dataseries produced by the missions discussed above). Thus a pipeline of model information guides the definition of the global scale models from which in turn local impact models can be derived.

For global climate change model data there are a number of sources available the most wellknown being those from the Intergovernmental Panel on Climate Change (IPCC).

The **Geophysical Fluid Dynamics Laboratory (GFDL)** develops and use dynamical, numerical models and computer simulations to improve our understanding and make projections of the behavior of the atmosphere, the oceans, and climate, using supercomputer and data storage resources. These models have become key tools to understand the physical and biogeochemical processes that control the earth’s climate

The **Coupled Model Intercomparison Project (CMIP)** is an international effort to improve climate models by comparing multiple model simulations to observations and to each other. Information can be found on the [CMIP-version 6 webpage](https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6). 

The CMIP and GDFL models are the primary sources for the IPCC reports.

For global domain-specifc model data there are a number of sources available:

* **MERRA-2** [EarthData](https://disc.gsfc.nasa.gov/datasets?project=MERRA-2): Modern-Era Retrospective analysis for Research and Applications, Version 2 and it is the first model for long-term, global reanalysis to assimilate space-based observations of aerosols and represent their interactions with other physical processes in the climate system
* **GEOS**: Goddard Earth Observing System is a model that spans a large range of space and time scales, and the modular structure of the models encompasses the representation of dynamical, physical, chemical, and biological processes.



## Accessing Earth Observations Data for Analysis

Now that we have a solid understanding of the data sphere around the NASA missions and the data gathered as well as what the role of models are in the decision and policy making process we can switch to the more technical part of this publication; "**How do we get access to the collected measurements and model results?**"

As I mentioned earlier there is a movement going on to consolidate collected imagery in Cloud based repositories; like the Microsoft ..., the Google Earth Engine and Amazon ... repositories. By now only these very large companies can collect the many pet-bytes of data collected almost daily by all the satellite intruments and the results from daily climate model runs.

In this section of the document I will address how we can get access to data providers without just yet addressing the type of information that can be extracted from these repositories. Typically there is a registration process that one must go through before accessing the data and then there are different processes to actually extract the data from the repositories. Once authorized to access a provider's data the process to access specific data sets is the same; typically a REST API or a data-order process, or a visual selection and request process. 

The two main providers of mission data are:
    For NASA, a popular service is NASA’s Earth Observing System Data and Information System (EOSDIS).
    For the European Space Agency, visit the Sentinel Open Access Hub for registration.


I'll start the discussion with the access process for NASA's data sources.

### Accessing NASA Data Sources

Under the umberlla of the NASA Earth Observation System (EOS) a large number of instituions manage the data collected as part of specific missions and sometime information from a single mission can be accessed through multiple agensies or institutions.

To streamline this process NASA has implemented a "single sign-on" process where a single account with the EOS system will get you access to multiple agency and institutions data repositories.

### Data search portal mainly
**NASA Open Data Portal**: <br/>
sign up at [https://data.nasa.gov/signup](https://data.nasa.gov/signup) and <br/>
sign in at [https://data.nasa.gov/login](https://data.nasa.gov/login)<br/>.
DATA.NASA.GOV is NASA's clearinghouse site for open-data provided to the public. Tens of thousands of datasets are available. The majority of dataset pages on data.nasa.gov only hold metadata for each dataset. It is common for the actual data to be held on other NASA archive sites. Data.nasa.gov will have the metadata and links to the data as it exists in those many other locations.

### Data access by API portal

**NASA Earthdata Portal**: <br/>
sign up at [https://www.earthdata.nasa.gov](https://www.earthdata.nasa.gov/) and 
<br/>sign in at [https://urs.earthdata.nasa.gov](https://urs.earthdata.nasa.gov/).<br/>
Earthdata Login is a user registration and user profile management system for users getting Earth science data from any of the Distributed Active Archive Centers (DAACs) that comprise NASA's Earth Observing System Data and Information System (EOSDIS). With the account registration out of the way we can start accessing the various data sets provided through the NASA missions.

Since this is a"single sign-on" your are providing, authorizing applications to use your account. This is a collection that grows as you access/use more data repositories and tools. Each time you want to access information from an agency you have never visited/authorized before you add their application to the list. For instance to access MERRIS-2 climate model data from the Goddard Earth Sciences Data and Information Services Center - GESDISC Test Data Archive you have to add them to your profile. 

This profile page is also where you requtest for access tokens that are needed to authenticate yourself when using an API.

* https://urs.earthdata.nasa.gov/users/raynierx/authorized_apps
* https://urs.earthdata.nasa.gov/users/raynierx/user_tokens

**How to Generate Earthdata Prerequisite Files
Overview**

This [guide demonstrates](https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Generate%20Earthdata%20Prerequisite%20Files) how to generate three Earthdata prerequisite files (.netrc, .urs_cookies, .dodsrc) needed to access the GES DISC archives through Python. It uses native Python libraries to generate the files in either Windows/macOS/Linux environments and to place them in their appropriate folders.
Create prerequisite files using Python
 
The following Python code will prompt you for your Earthdata username and password before generating and storing these three files in their appropriate locations for Windows, Linux, or macOS operating systems. It can be run multiple times, and each time will overwrite existing credential files (if they already exist). This code can be run inside of this notebook or can be put into its own Python script.

```python
from netrc import netrc        # std.lib
from subprocess import Popen   # std.lib
from getpass import getpass    # std.lib
import platform                # std.lib
import os                      # std.lib
import shutil                  # std.lib

urs = 'urs.earthdata.nasa.gov'    # Earthdata URL to call for authentication
prompts = ['Enter NASA Earthdata Login Username \n(or create an account at urs.earthdata.nasa.gov): ',
           'Enter NASA Earthdata Login Password: ']

homeDir = os.path.expanduser("~") + os.sep

with open(homeDir + '.netrc', 'w') as file:
    file.write('machine {} login {} password {}'.format(urs, getpass(prompt=prompts[0]), getpass(prompt=prompts[1])))
    file.close()
with open(homeDir + '.urs_cookies', 'w') as file:
    file.write('')
    file.close()
with open(homeDir + '.dodsrc', 'w') as file:
    file.write('HTTP.COOKIEJAR={}.urs_cookies\n'.format(homeDir))
    file.write('HTTP.NETRC={}.netrc'.format(homeDir))
    file.close()

print('Saved .netrc, .urs_cookies, and .dodsrc to:', homeDir)

# Set appropriate permissions for Linux/macOS
if platform.system() != "Windows":
    Popen('chmod og-rw ~/.netrc', shell=True)
else:
    # Copy dodsrc to working directory in Windows  
    shutil.copy2(homeDir + '.dodsrc', os.getcwd())
    print('Copied .dodsrc to:', os.getcwd())
```

**Running Wget Commands with the Subprocess Package**

To run Wget commands from within a Python script, you’ll use the Popen method of the subprocess package. Every time your script invokes popen(), it will execute the command you passed in an independent instance of the operating system’s command processor. By setting the verbose argument to True, it will also return the output of the command.  https://colab.research.google.com/drive/1VyWXq_RAqeq-6UIiiM-3vpyKpIozKvsL?usp=sharing#scrollTo=21mwgF1E7ZtB

```python
import subprocess

def runcmd(cmd, verbose = False, *args, **kwargs):

    process = subprocess.Popen(
        cmd,
        stdout = subprocess.PIPE, 
        stderr = subprocess.PIPE,
        text = True, 
        shell = True
    )
    std_out, std_err = process.communicate()
    if verbose:
        print(std_out.strip(), std_err)
    pass

    
runcmd('echo "Hello, World!"', verbose = True)

runcmd("wget https://www.scrapingbee.com/images/logo-small.png", verbose = True)



#### Access LANDSAT missions datasets
The process to access LANDSAT information through the NASA DAACs we need to login to our EarthData account first. Information on how to access the LANDSAT datasets can be fonud at the [following page](https://www.usgs.gov/landsat-missions/landsat-data-access). 

The USGS process is a cumbersome process and there are easier ways to get to the imagery through open source packages. All user **<span style="color:darkorange">interactions with this API functions must be accompanied by valid credentials!</span>**. Accounts are managed in the [USGS EROS Registration system](https://ers.cr.usgs.gov/register). At this url you can request for a free API account.

<br/>

#### <span style="color:purple;font-weight:bold;">Easy Landsat Download</span>
[Easy Landsat Download](https://github.com/dgketchum/Landsat578) This package is easy to use within a python program:

```Python
from landsat.google import GoogleDownload

g = GoogleDownload(start='2015-01-01', end='2015-12-31', sat=8, path='38', row='28', output_path='path/to/place')
g.download()
```
It would seem this is the easiest of the packages so far and it has fairly good documentation. As a google-based product there could be some GEE account registration requirements.


<br/><br/><br/>
####  <span style="color:purple;font-weight:bold;">landsatxplore Python package
The **[landsatxplore Python package](https://github.com/yannforget/landsatxplore)** provides an interface to the USGS EarthExplorer portal to search and download Landsat Collections scenes through a command-line interface or a Python API. 

Note that this package doesn't talk to the GoogleEarth API but to the source USGS API. This might mean that the API can be developed to access other products in the USGS repository?

The following datasets are supported:

|Dataset Name |	Dataset ID |
|-------------|------------|
| Landsat 5 TM Collection 1 Level 1	| landsat_tm_c1 |
| Landsat 5 TM Collection 2 Level 1	| landsat_tm_c2_l1 |
| Landsat 5 TM Collection 2 Level 2	| landsat_tm_c2_l2 |
| Landsat 7 ETM+ Collection 1 Level 1	| landsat_etm_c1 |
| Landsat 7 ETM+ Collection 2 Level 1	| landsat_etm_c2_l1 |
| Landsat 7 ETM+ Collection 2 Level 2	| landsat_etm_c2_l2 |
| Landsat 8 Collection 1 Level 1	| landsat_8_c1 |
| Landsat 8 Collection 2 Level 1	| landsat_ot_c2_l1 |
| Landsat 8 Collection 2 Level 2	| landsat_ot_c2_l2 |
| Sentinel 2A	| sentinel_2a |

This API provides the following interface:

```Python
# Searching for Landsat 5 TM scenes that contains the location (12.53, -1.53) acquired during the year 1995.

landsatxplore search --dataset LANDSAT_TM_C1 --location 12.53 -1.53 \
    --start 1995-01-01 --end 1995-12-31
```

<br/><br/><br/>
#### <span style="color:purple;font-weight:bold;">EOReader</span>
[EOReader](https://github.com/sertit/eoreader) is a remote-sensing opensource python library reading optical and SAR constellations, loading and stacking bands, clouds, DEM and spectral indices in a sensor-agnostic way.

There are a number of notebooks available. This package focusses more on the processing of the files than on the search and retrieval. To get access and download the files another package is needed if you want to do this in Python code.

EOReader works with xarrays.DataArray and geopandas.GeoDataFrames and seems mature in the geo-packages it uses: large data szie processing etc. The package supports a large number of satellite formats:

|Optical	|SAR |
|-----------|---|
| Landsat 1 to 9| Sentinel-1 |
| Sentinel-2 and Sentinel-2 Theia| COSMO-Skymed 1st and 2nd Generation
| Sentinel-3 OLCI and SLSTR| TerraSAR-X, TanDEM-X and PAZ SAR
| PlanetScope and SkySat| RADARSAT-2 and RADARSAT-Constellation
| Pleiades and Pleiades-Neo| ICEYE
| SPOT-6/7| SAOCOM |
| SPOT-4/5| |
| Vision-1| |
| Maxar (WorldViews, GeoEye)| |
| SuperView-1| |

It also implements additional sensor-agnostic features:
* load: Load many band types:
  * satellite bands (optical or SAR)
  * index
  * cloud bands
  * DEM bands
* stack: Stack all these type of bands


<br/><br/><br/>
#### <span style="color:purple;font-weight:bold;">The sentinelhub Python package</span>
The [sentinelhub Python package](https://github.com/sentinel-hub/sentinelhub-py) is the official Python interface for Sentinel Hub services. It supports most of the services described in the Sentinel Hub documentation and any type of satellite data collections, including Sentinel, Landsat, MODIS, DEM, and custom collections produced by users.

The package also provides a collection of basic tools and utilities for working with geospatial and satellite data. It builds on top of well known packages such as numpy, shapely, pyproj, etc. It is also a core dependency of eo-learn Python package for creating geospatial data-processing workflows.

The main package resources are GitHub repository, documentation page, and [Sentinel Hub forum](https://forum.sentinel-hub.com/top?period=quarterly)

https://docs.sentinel-hub.com/api/latest/data/

* Sentinel-1, Sentinel-2 L1C and L2A, Sentinel-3 OLCI and SLSTR, Sentinel-5P
* Landsat 9-8 OLI-TIRS, Landsat 7 ETM, Landsat 4-5 TM and Landsat 1-5 MSS
* Airbus SPOT and Pleiades, PlanetScope and Maxar WorldView/GeoEye
* Mapzen and Copernicus digital elevation models
* Envisat collection
* Moderate Resolution Imaging Spectroradiometer (MODIS) MCD43A4, version 6
* Several Copernicus Services collections, such as Corine Land Cover, Global Land cover and Water Bodies


<br/><br/><br/>
#### <span style="color:purple;font-weight:bold;">EODAG (Earth Observation Data Access Gateway</span>
[EODAG (Earth Observation Data Access Gateway)](https://github.com/CS-SI/eodag) is a command line tool and a plugin-oriented Python framework for searching, aggregating results and downloading remote sensed images while offering a unified API for data access regardless of the data provider. This function is both an API and a command line utility. 

>NOTE: product seems overkill for smaller applications.

The EODAG SDK is structured around three functions:

* List product types: list of supported products and their description
* Search products (by product type or uid) : searches products according to the search criteria provided
* Download products : download product “as is"

The package seems to be very well supported and documented. For the Python API documentation go to [Python API User Guide](https://eodag.readthedocs.io/en/latest/api_user_guide.html). The site has some ready to use JJupyter Notebooks to start with.


<br/><br/><br/>
####  <span style="color:purple;font-weight:bold;">landsat-util
This package provides similar search and retrieve functionality but has some image processing features for image enhancements. Documentation can be found at [landsat-util documentation](https://pythonhosted.org/landsat-util/overview.html).


This API provides the following interface:

```Python
# Search by path and row:

$: landsat search --cloud 4 --start "january 1 2014" --end "january 10 2014" -p 009,045
```



#### Access AQUA and TERRA missions datasets
To access the LANDSAT datasets we need to login to our EarthData account first.

https://search.earthdata.nasa.gov/search

The information made available through the AQUA/TERRA missions is disseminated based on the instruments on the satelites. For the AQUA mission that are the:

Aqua carries six state-of-the-art instruments in a near-polar low-Earth orbit. The six instruments are the
* **Atmospheric Infrared Sounder (AIRS)**, the
* **Advanced Microwave Sounding Unit (AMSU-A)**, the
* **Humidity Sounder for Brazil (HSB)**, the
* **Advanced Microwave Scanning Radiometer for EOS (AMSR-E)**, the
* **Moderate-Resolution Imaging Spectroradiometer (MODIS)**, and
* **Clouds and the Earth's Radiant Energy System (CERES)**. 

On the https://disc.gsfc.nasa.gov/datasets page you'll find in the left nav bar an option to specify the source of the data. This corresponds to the instrument that generated the data.

**<div style="color:red">THIS IS A MASSIVE LIST OF INSTRUMENTS!!**</div>

Another long list of instrument data is found at: https://asdc.larc.nasa.gov/data where CERES data can be found. CERES is not available on GDISC.


##### AIRS
https://disc.gsfc.nasa.gov/datasets?keywords=AIRS%2BNRT&page=1

##### AMSU-A
https://www.ncei.noaa.gov/data/amsu-a-brightness-temperature-noaa/access/

#### HSB
https://disc.gsfc.nasa.gov/datasets?keywords=HSB&page=1


Each has unique characteristics and capabilities, and all six serve together to form a powerful package for Earth observation

Terra collects data about the Earth’s bio-geochemical and energy systems using five sensors that observe the atmosphere, land surface, oceans, snow and ice, and energy budget. Each sensor has unique features that enable scientists to meet a wide range of science objectives. The five Terra onboard instruments are:
* **Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER)**,
* **Clouds and Earth's Radiant Energy System (CERES)**,
* **Measurements of Pollution in the Troposphere (MOPITT)**,
* **Moderate Resolution Imaging Spectroradiometer (MODIS)**, and
* **Multi-angle Imaging Spectroradiometer (MISR)**.

- https://terra.nasa.gov/data/aster-data: EARTHDATA, GDISC
- https://terra.nasa.gov/data/ceres-data: ASDC
- https://terra.nasa.gov/data/misr-data: ASDC 
- https://terra.nasa.gov/data/modis-data: 
- https://terra.nasa.gov/data/mopitt-data: EARTHDATA, ASDC


To download a single file:
1. Copy the example script. 
2. Write the example script text to a file ending in .py. 
3. Advanced users may want to edit the code to add additional features and advanced filtering
4. Select a top level URL. 
5. Search https://asdc.larc.nasa.gov/data/ for a directory or file that contains the data you want to download.
6. Find your Earthdata login token. You can generate a token or copy an existing token by going to https://urs.earthdata.nasa.gov/ and selecting the "Generate Token" option form the top menu. Please note the token's expiration date, after that date a new token must be generated.
7. Add your URL and token to the script.
8. Run the script.

[Downloading ASDC Data with Python 3](https://forum.earthdata.nasa.gov/viewtopic.php?t=2330)

The forum discusses issues and provides a number of possible solutions.

```python
import requests
from pathlib import Path

url=<the URL of the file you want to download>
token=<your token>
header={"Authorization": f"Bearer {token}"}

response = requests.get(url, headers=header)
content = response.content
file_name = url.split('/')[-1]
data_path = Path(file_name)
data_path.write_bytes(content)
```

## References to Satellite and Instruments

Here you can find a few useful Python libraries that can be used to automatically fetch data from specific satellite products:

* Sentinel: [SentinelHub](https://github.com/sentinel-hub/sentinelhub-py)
* Landsat: [landsatxplore](https://github.com/yannforget/landsatxplore)
* Modis: [pyModis](http://www.pymodis.org/) **too complex**
* GOES: [SatPy](https://github.com/pytroll/satpy) **GOES-16/17 NOAA Geostationary Operational Environmental Satellite data**
* GPM: [nasadap](https://github.com/mullenkamp/nasadap) **too complex**


## References to NASA EarthData Data and Toolkist

https://www.earthdata.nasa.gov/learn/find-data
https://www.earthdata.nasa.gov/learn/toolkits
